{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "# открываю текст по url\n",
    "html = urlopen(\"http://lib.ru/ZELQZNY/zheliazny_hronomaster.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import re\n",
    "text = \" \".join([re.sub('[^А-Яа-я.!?]', \" \", line.decode(html.headers.get_content_charset())) for line in html])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = list(word_tokenize(re.sub('[,:;.!?]', \" \", sent.lower())) for sent in sent_tokenize(text))\n",
    "n = 2\n",
    "n_grams, words = padded_everygram_pipeline(n, sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE - 0.017543859649122806\n",
      "Lidstone лямбда 0.1 - 0.0006999236446933062\n",
      "Lidstone лямбда 1 - 0.0001315529829638887\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "\n",
    "modelMLE = MLE(n)\n",
    "modelMLE.fit(n_grams, words)\n",
    "print(f'MLE - {modelMLE.score(\"замечательно\", [\"просто\"])}')\n",
    "\n",
    "from nltk.lm import Lidstone\n",
    "\n",
    "def lidstones(l):\n",
    "    n_grams_lidstone, words_lidstone = padded_everygram_pipeline(n, sentences)\n",
    "    model_lidstone = Lidstone(l, n)\n",
    "    model_lidstone.fit(n_grams_lidstone, words_lidstone)\n",
    "    print(f'Lidstone лямбда {l} - {model_lidstone.score(\"замечательно\", [\"просто\"])}')\n",
    "\n",
    "lidstones(0.1)\n",
    "lidstones(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lidstone лямбда = 0.01, n = 1\n",
      "1519.593352284368\n",
      "--------------------\n",
      "Lidstone лямбда = 0.01, n = 2\n",
      "66.4558890776621\n",
      "--------------------\n",
      "Lidstone лямбда = 0.01, n = 3\n",
      "53.75412458281915\n",
      "--------------------\n",
      "Lidstone лямбда = 0.01, n = 4\n",
      "53.89517135499302\n",
      "--------------------\n",
      "Lidstone лямбда = 0.01, n = 5\n",
      "53.74511692678656\n",
      "--------------------\n",
      "Lidstone лямбда = 0.05, n = 1\n",
      "1519.74976227643\n",
      "--------------------\n",
      "Lidstone лямбда = 0.05, n = 2\n",
      "215.58930039978077\n",
      "--------------------\n",
      "Lidstone лямбда = 0.05, n = 3\n",
      "222.99315288115548\n",
      "--------------------\n",
      "Lidstone лямбда = 0.05, n = 4\n",
      "226.83479799562454\n",
      "--------------------\n",
      "Lidstone лямбда = 0.05, n = 5\n",
      "228.05765474228403\n",
      "--------------------\n",
      "Lidstone лямбда = 0.1, n = 1\n",
      "1520.425629144585\n",
      "--------------------\n",
      "Lidstone лямбда = 0.1, n = 2\n",
      "373.38429516013736\n",
      "--------------------\n",
      "Lidstone лямбда = 0.1, n = 3\n",
      "407.86607755041064\n",
      "--------------------\n",
      "Lidstone лямбда = 0.1, n = 4\n",
      "416.25198839857546\n",
      "--------------------\n",
      "Lidstone лямбда = 0.1, n = 5\n",
      "419.5554597186508\n",
      "--------------------\n",
      "Lidstone лямбда = 0.2, n = 1\n",
      "1523.1366196782749\n",
      "--------------------\n",
      "Lidstone лямбда = 0.2, n = 2\n",
      "645.9941082754419\n",
      "--------------------\n",
      "Lidstone лямбда = 0.2, n = 3\n",
      "726.4389362085515\n",
      "--------------------\n",
      "Lidstone лямбда = 0.2, n = 4\n",
      "742.6335621679294\n",
      "--------------------\n",
      "Lidstone лямбда = 0.2, n = 5\n",
      "749.8244099081867\n",
      "--------------------\n",
      "Lidstone лямбда = 0.5, n = 1\n",
      "1538.9150699336851\n",
      "--------------------\n",
      "Lidstone лямбда = 0.5, n = 2\n",
      "1266.1323764033386\n",
      "--------------------\n",
      "Lidstone лямбда = 0.5, n = 3\n",
      "1434.106817504679\n",
      "--------------------\n",
      "Lidstone лямбда = 0.5, n = 4\n",
      "1465.3750610284783\n",
      "--------------------\n",
      "Lidstone лямбда = 0.5, n = 5\n",
      "1480.7121097768854\n",
      "--------------------\n",
      "Lidstone лямбда = 1, n = 1\n",
      "1578.6720122886577\n",
      "--------------------\n",
      "Lidstone лямбда = 1, n = 2\n",
      "1941.651683846744\n",
      "--------------------\n",
      "Lidstone лямбда = 1, n = 3\n",
      "2171.9347357033603\n",
      "--------------------\n",
      "Lidstone лямбда = 1, n = 4\n",
      "2213.905202900232\n",
      "--------------------\n",
      "Lidstone лямбда = 1, n = 5\n",
      "2235.741277841025\n",
      "--------------------\n",
      "Lidstone лямбда = 2, n = 1\n",
      "1673.3550571643143\n",
      "--------------------\n",
      "Lidstone лямбда = 2, n = 2\n",
      "2707.200459069157\n",
      "--------------------\n",
      "Lidstone лямбда = 2, n = 3\n",
      "2961.999913635398\n",
      "--------------------\n",
      "Lidstone лямбда = 2, n = 4\n",
      "3008.162779219621\n",
      "--------------------\n",
      "Lidstone лямбда = 2, n = 5\n",
      "3033.6189263139026\n",
      "--------------------\n",
      "Lidstone лямбда = 5, n = 1\n",
      "1941.5313829578954\n",
      "--------------------\n",
      "Lidstone лямбда = 5, n = 2\n",
      "3635.0167775562027\n",
      "--------------------\n",
      "Lidstone лямбда = 5, n = 3\n",
      "3848.579216203096\n",
      "--------------------\n",
      "Lidstone лямбда = 5, n = 4\n",
      "3888.062036607378\n",
      "--------------------\n",
      "Lidstone лямбда = 5, n = 5\n",
      "3911.6572214164084\n",
      "--------------------\n",
      "Kneser-Ney лямбда = 0.01, n = 1\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mZeroDivisionError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 47>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     45\u001B[0m n_list \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]\n\u001B[0;32m     46\u001B[0m [test_models(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLidstone\u001B[39m\u001B[38;5;124m'\u001B[39m, l, n_var) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;241m0.05\u001B[39m, \u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m0.2\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m5\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m n_var \u001B[38;5;129;01min\u001B[39;00m n_list]\n\u001B[1;32m---> 47\u001B[0m [test_models(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKneser-Ney\u001B[39m\u001B[38;5;124m'\u001B[39m, l, n_var) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;241m0.02\u001B[39m, \u001B[38;5;241m0.05\u001B[39m, \u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m0.2\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.95\u001B[39m, \u001B[38;5;241m0.99\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m n_var \u001B[38;5;129;01min\u001B[39;00m n_list]\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mПобедитель!\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     50\u001B[0m test_models(best_model[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m], best_model[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlambda\u001B[39m\u001B[38;5;124m'\u001B[39m], best_model[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     45\u001B[0m n_list \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]\n\u001B[0;32m     46\u001B[0m [test_models(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLidstone\u001B[39m\u001B[38;5;124m'\u001B[39m, l, n_var) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;241m0.05\u001B[39m, \u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m0.2\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m5\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m n_var \u001B[38;5;129;01min\u001B[39;00m n_list]\n\u001B[1;32m---> 47\u001B[0m [\u001B[43mtest_models\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mKneser-Ney\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ml\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_var\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;241m0.02\u001B[39m, \u001B[38;5;241m0.05\u001B[39m, \u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m0.2\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.95\u001B[39m, \u001B[38;5;241m0.99\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m n_var \u001B[38;5;129;01min\u001B[39;00m n_list]\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mПобедитель!\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     50\u001B[0m test_models(best_model[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m], best_model[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlambda\u001B[39m\u001B[38;5;124m'\u001B[39m], best_model[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36mtest_models\u001B[1;34m(model_name, l, n_size, flag_test)\u001B[0m\n\u001B[0;32m     32\u001B[0m     model_kneserney \u001B[38;5;241m=\u001B[39m KneserNeyInterpolated(n_size, l)\n\u001B[0;32m     33\u001B[0m     model_kneserney\u001B[38;5;241m.\u001B[39mfit(n_grams_train, words_train)\n\u001B[1;32m---> 34\u001B[0m     perplexity \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_kneserney\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mperplexity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_grams_flat\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m(best_model[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mperplexity\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;129;01mor\u001B[39;00m perplexity \u001B[38;5;241m<\u001B[39m best_model[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mperplexity\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m     37\u001B[0m     best_model[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mperplexity\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m perplexity\n",
      "File \u001B[1;32m~\\Обработка текстов\\lib\\site-packages\\nltk\\lm\\api.py:173\u001B[0m, in \u001B[0;36mLanguageModel.perplexity\u001B[1;34m(self, text_ngrams)\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mperplexity\u001B[39m(\u001B[38;5;28mself\u001B[39m, text_ngrams):\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;124;03m\"\"\"Calculates the perplexity of the given text.\u001B[39;00m\n\u001B[0;32m    169\u001B[0m \n\u001B[0;32m    170\u001B[0m \u001B[38;5;124;03m    This is simply 2 ** cross-entropy for the text, so the arguments are the same.\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \n\u001B[0;32m    172\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mpow\u001B[39m(\u001B[38;5;241m2.0\u001B[39m, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mentropy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext_ngrams\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\Обработка текстов\\lib\\site-packages\\nltk\\lm\\api.py:164\u001B[0m, in \u001B[0;36mLanguageModel.entropy\u001B[1;34m(self, text_ngrams)\u001B[0m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mentropy\u001B[39m(\u001B[38;5;28mself\u001B[39m, text_ngrams):\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;124;03m\"\"\"Calculate cross-entropy of model for given evaluation text.\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \n\u001B[0;32m    159\u001B[0m \u001B[38;5;124;03m    :param Iterable(tuple(str)) text_ngrams: A sequence of ngram tuples.\u001B[39;00m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;124;03m    :rtype: float\u001B[39;00m\n\u001B[0;32m    161\u001B[0m \n\u001B[0;32m    162\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m*\u001B[39m _mean(\n\u001B[1;32m--> 164\u001B[0m         [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogscore(ngram[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], ngram[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m ngram \u001B[38;5;129;01min\u001B[39;00m text_ngrams]\n\u001B[0;32m    165\u001B[0m     )\n",
      "File \u001B[1;32m~\\Обработка текстов\\lib\\site-packages\\nltk\\lm\\api.py:164\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mentropy\u001B[39m(\u001B[38;5;28mself\u001B[39m, text_ngrams):\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;124;03m\"\"\"Calculate cross-entropy of model for given evaluation text.\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \n\u001B[0;32m    159\u001B[0m \u001B[38;5;124;03m    :param Iterable(tuple(str)) text_ngrams: A sequence of ngram tuples.\u001B[39;00m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;124;03m    :rtype: float\u001B[39;00m\n\u001B[0;32m    161\u001B[0m \n\u001B[0;32m    162\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m*\u001B[39m _mean(\n\u001B[1;32m--> 164\u001B[0m         [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogscore\u001B[49m\u001B[43m(\u001B[49m\u001B[43mngram\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mngram\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m ngram \u001B[38;5;129;01min\u001B[39;00m text_ngrams]\n\u001B[0;32m    165\u001B[0m     )\n",
      "File \u001B[1;32m~\\Обработка текстов\\lib\\site-packages\\nltk\\lm\\api.py:143\u001B[0m, in \u001B[0;36mLanguageModel.logscore\u001B[1;34m(self, word, context)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlogscore\u001B[39m(\u001B[38;5;28mself\u001B[39m, word, context\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;124;03m\"\"\"Evaluate the log score of this word in this context.\u001B[39;00m\n\u001B[0;32m    139\u001B[0m \n\u001B[0;32m    140\u001B[0m \u001B[38;5;124;03m    The arguments are the same as for `score` and `unmasked_score`.\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \n\u001B[0;32m    142\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m log_base2(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\Обработка текстов\\lib\\site-packages\\nltk\\lm\\api.py:117\u001B[0m, in \u001B[0;36mLanguageModel.score\u001B[1;34m(self, word, context)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mscore\u001B[39m(\u001B[38;5;28mself\u001B[39m, word, context\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;124;03m\"\"\"Masks out of vocab (OOV) words and computes their model score.\u001B[39;00m\n\u001B[0;32m    113\u001B[0m \n\u001B[0;32m    114\u001B[0m \u001B[38;5;124;03m    For model-specific logic of calculating scores, see the `unmasked_score`\u001B[39;00m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;124;03m    method.\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munmasked_score\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlookup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlookup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[0;32m    119\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Обработка текстов\\lib\\site-packages\\nltk\\lm\\models.py:104\u001B[0m, in \u001B[0;36mInterpolatedLanguageModel.unmasked_score\u001B[1;34m(self, word, context)\u001B[0m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munmasked_score\u001B[39m(\u001B[38;5;28mself\u001B[39m, word, context\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    102\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m context:\n\u001B[0;32m    103\u001B[0m         \u001B[38;5;66;03m# The base recursion case: no context, we only have a unigram.\u001B[39;00m\n\u001B[1;32m--> 104\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munigram_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    105\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcounts[context]:\n\u001B[0;32m    106\u001B[0m         \u001B[38;5;66;03m# It can also happen that we have no data for this context.\u001B[39;00m\n\u001B[0;32m    107\u001B[0m         \u001B[38;5;66;03m# In that case we defer to the lower-order ngram.\u001B[39;00m\n\u001B[0;32m    108\u001B[0m         \u001B[38;5;66;03m# This is the same as setting alpha to 0 and gamma to 1.\u001B[39;00m\n\u001B[0;32m    109\u001B[0m         alpha, gamma \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\Обработка текстов\\lib\\site-packages\\nltk\\lm\\smoothing.py:98\u001B[0m, in \u001B[0;36mKneserNey.unigram_score\u001B[1;34m(self, word)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munigram_score\u001B[39m(\u001B[38;5;28mself\u001B[39m, word):\n\u001B[0;32m     97\u001B[0m     word_continuation_count, total_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_continuation_counts(word)\n\u001B[1;32m---> 98\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mword_continuation_count\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtotal_count\u001B[49m\n",
      "\u001B[1;31mZeroDivisionError\u001B[0m: division by zero"
     ]
    }
   ],
   "source": [
    "from nltk.lm import KneserNeyInterpolated\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "import random\n",
    "\n",
    "random.shuffle(sentences)\n",
    "train_set = sentences[math.floor(len(sentences) * 0.8):]\n",
    "validation_set = sentences[math.floor(len(sentences) * 0.8):math.floor(len(sentences) * 0.9)]\n",
    "test_set = sentences[:math.floor(len(sentences) * 0.9)]\n",
    "best_model = {\n",
    "    'perplexity': None,\n",
    "    'n' : None,\n",
    "    'lambda' : None,\n",
    "    'name' : None\n",
    "}\n",
    "\n",
    "def test_models(model_name: str, l: float, n_size: int, flag_test: bool = False) -> None:\n",
    "    global perplexity\n",
    "    n_grams_train, words_train = padded_everygram_pipeline(n_size, train_set)\n",
    "    current_set = test_set if flag_test else validation_set\n",
    "    n_grams = list(ngrams(pad_both_ends(sent, n_size), n_size) for sent in current_set)\n",
    "    n_grams_flat = list(item for sublist in n_grams for item in sublist)\n",
    "    print(f'{model_name} лямбда = {l}, n = {n_size}')\n",
    "\n",
    "    if (model_name == \"Lidstone\"):\n",
    "        model_lidstone = Lidstone(l, n_size)\n",
    "        model_lidstone.fit(n_grams_train, words_train)\n",
    "        perplexity = model_lidstone.perplexity(n_grams_flat)\n",
    "\n",
    "    if (model_name == \"Kneser-Ney\"):\n",
    "        model_kneserney = KneserNeyInterpolated(n_size, l)\n",
    "        model_kneserney.fit(n_grams_train, words_train)\n",
    "        perplexity = model_kneserney.perplexity(n_grams_flat)\n",
    "\n",
    "    if not(best_model['perplexity']) or perplexity < best_model['perplexity']:\n",
    "        best_model['perplexity'] = perplexity\n",
    "        best_model['n'] = n_size\n",
    "        best_model['lambda'] = l\n",
    "        best_model['name'] = model_name\n",
    "\n",
    "    print(perplexity)\n",
    "    print('--------------------')\n",
    "\n",
    "n_list = [1, 2, 3, 4, 5]\n",
    "[test_models('Lidstone', l, n_var) for l in [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 5] for n_var in n_list]\n",
    "[test_models('Kneser-Ney', l, n_var) for l in [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 0.95, 0.99] for n_var in n_list]\n",
    "\n",
    "print('Победитель!')\n",
    "test_models(best_model['name'], best_model['lambda'], best_model['n'], True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9995419",
   "language": "python",
   "display_name": "PyCharm (Обработка текстов)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}